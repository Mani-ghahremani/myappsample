{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de35f6c-ccf4-4568-b25b-181383eea769",
   "metadata": {},
   "source": [
    "# Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28413cc0-6868-42ff-9db7-0ba6ae70a1f8",
   "metadata": {},
   "source": [
    "The more the telecom services marketing paradigm evolves, the more important it becomes to retain high value customers. Traditional customer segmentation methods based on experience or ARPU (Average Revenue per User) consider neither customers’ future revenue nor the cost of servicing customers of different types. Therefore, it is very difficult to effectively identify high-value customers.\n",
    "\n",
    "Cons:\n",
    "- Current segmentation serves various areas of the company aiming different objectives (best practices have different segmentations for different purposes such as customer care, acquisition and offering P&S)\n",
    "- Mixes behavioural elements with customer value\n",
    "- Leaves out elements describing customer behaviour e.g. recharge behaviour, payment methods\n",
    "- Does not link to the market or market potential; looks only at MCI customer base\n",
    "- Does not incorporate future needs/ potential; captures only historical/ current customer usage\n",
    "\n",
    " MCI’s current segmentation is ARPU-based and does not allow value-based marketing. MCI’s current segmentation approach\n",
    "- SHVC(Super high value customers)\n",
    "- HVR(High voice revenue)\n",
    "- HMR(High mix revenue)\n",
    "- HDR(High data revenue)\n",
    "- Mass\n",
    "\n",
    "\n",
    "Ideally, different segmentation techniques should be used for different purposes such as customer care, acquisition and P&S development\n",
    "1. ARPU and lifetime value based segmentation for CC(ARPU and life-time customer value based segmentation (across all touchpoints))\n",
    "2. Micro-segmentation for P&S development(Dynamic/ Micro segmentation (contextual marketing to target microsegments) against traditional statistic segmentation\n",
    "3. Value based segmentation  for customer acquisitions(Value based segmentation that mixes behaviour patterns and values to address specific target segments with tailor-made offers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2656ce-ebb3-4e07-854a-0e2b55b3e331",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3103d1-7b42-4cce-b401-f65b81836e46",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615d8c9-50fb-4821-b20f-8a85f5e81bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy import percentile\n",
    "from IPython.display import Audio\n",
    "from sklearn.ensemble import IsolationForest\n",
    "sound='0.mp3'\n",
    "#%%capture #run the cell, capturing stdout, stderr, and IPython’s rich display() calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967a82c-6eb4-4be8-91e9-fe1f923e8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('100000.csv', sep=',', dtype={'CUSTOMER_ID': float, 'GENDER': float })\n",
    "print(type(data)) # pandas.core.frame.DataFrame\n",
    "backup=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006c0bc-71ff-4d29-b7a5-d0d3c97a183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95a60e-131b-4297-9b86-5dff3d7da703",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "null_values=data.columns[data.isnull().any()]\n",
    "data[null_values].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ddecd-0885-4bad-8d39-9810ec779562",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc4a22-5356-4178-b337-ccd4b9ed6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[16443,:]  # we can see ### input for AGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79eb0a4-13f3-47ed-a0b4-6d289f3fd04a",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29531e8e-f439-482d-b6f3-c48d1bc268da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['COUNT_MSISDN'].fillna(0,inplace=True)\n",
    "data['GENDER'].fillna(0,inplace=True) #222\n",
    "\n",
    "\n",
    "data['AGE'].fillna(0,inplace=True) #222\n",
    "data['AGE'].replace('###',0,inplace=True)\n",
    "\n",
    "\n",
    "# df[\"column1\"].replace({\"a\": \"x\", \"b\": \"y\"}, inplace=True)\n",
    "\n",
    "\n",
    "#convert GENERATION to number\n",
    "data['GENERATION'].fillna('0G',inplace=True) \n",
    "data['GENERATION']= data['GENERATION'].str.split('G' , expand=True,)\n",
    "\n",
    "# data['GENERATION']= data['GENERATION'].str.strip().split('g')[0]\n",
    "# data['GENERATION']= data['GENERATION'].values.strip().split('g')[0]\n",
    "\n",
    "#.split('g')[0]\n",
    "\n",
    "#Voice_main_lac\n",
    "data['VOICE_MAIN_LACCELL'].fillna(0,inplace=True) \n",
    "data['VOICE_MAIN_LAC_CELL_PROVINCE'].fillna(0,inplace=True) \n",
    "data['ACTIVE_INTEC_USAGE'].fillna(0,inplace=True) \n",
    "\n",
    "#data[data.VOICE_MAIN_LACCELL.isnull()]=-1\n",
    "#data[data.VOICE_MAIN_LAC_CELL_PROVINCE.isnull()]=-1\n",
    "#data[data.ACTIVE_INTEC_USAGE.isnull()]=-1\n",
    "\n",
    "\n",
    "#Packages\n",
    "data['MONTHLY_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['BOUNDLE_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['HOURLY_U_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['GIFT_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['HOURLY_L_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['SHARED_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['OTHER_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['CLEAN_INTERNET_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['CUSTOMIZED_OFFER_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['DTS_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['B2B_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['SHORT_TERM_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['LONG_TERM_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['NEWSUBS_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['CONTENT_PLAN_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['ROAMING_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['UNKNOWN_TYPE_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['ZZ_BLANK_GENERAL_TYPE_PKG_COUNT'].fillna(0,inplace=True)\n",
    "data['ZZ_NEED_TO_MAP_PKG_COUNT'].fillna(0,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6da2c-f666-421f-a306-b7545d7c6cb8",
   "metadata": {},
   "source": [
    "# Calculation fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecae7c2-8156-48f5-a40d-7beab49eadd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TENURE']=1400 - data.ACTIVATION_YEAR.values\n",
    "data['VOL_ALL_CBS_BYTE']=data['VOL_2G_CBS_BYTE'].values+ data['VOL_3G_CBS_BYTE'].values + data['VOL_4G_CBS_BYTE'].values+data['VOL_UNK_CBS_BYTE'].values\n",
    "data['ARPU_DATA']= data['CASH_PKG'].values + data['PAYG_DATA_RIAL'].values + data['PKG_DATA_ACTIVATION_RIAL_CRM'].values \n",
    "data['ARPU_NONDATA']=data.ARPU_RIAL.values - data.ARPU_DATA.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87bd917-2177-491d-aca0-b6a4497d8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.GENDER.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b969f7c-f714-4052-b5f7-f56d31db930b",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# EDA(Eploratory Data Analysis)\n",
    "Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161af25e-6602-4c20-8ea7-3ad3d21ecbe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-304fa4ce4ebd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f3138-34a7-444a-b947-780f89b8f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "# we see many Data types as objects, so we have to change them into the proper Data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ea563-3b36-4d94-b33e-3bab075d2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe() #Looking for some statistical information about each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e437ee-da31-4292-85aa-b41b57459d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "sns.heatmap(data.corr(),cmap='viridis', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858674d8-65c5-437f-b85c-1f3e4385809b",
   "metadata": {},
   "source": [
    "### Checking the skewness of our dataset.\n",
    "- A normally distribuited data has a skewness close to zero.\n",
    "- Skewness greather than zero means that there is more weight in the left side of the data.\n",
    "- In another hand, skewness smaller than 0 means that there is more weight in the right side of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e558bf2d-4cd7-4c30-a335-3033784c40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c192b4-8fca-4d45-8cce-9e3d69a5bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['GENDER'] = pd.to_numeric(data['GENDER'])\n",
    "# data['GENDER'] =[int(g) for g in data['GENDER']]\n",
    "data['GENDER'] = data['GENDER'].astype('int8')\n",
    "data['AGE'] = data['AGE'].astype('int8')\n",
    "# data['GENERATION']= [int(g) for g in data['GENERATION']]\n",
    "data['GENERATION']=data['GENERATION'].astype('int8')\n",
    "# data['PROVINCE']= [str(g) for g in data['PROVINCE']]          # data['PROVINCE'].astype(str)\n",
    "\n",
    "data['VAS_CNT'] = pd.to_numeric(data['VAS_CNT'])\n",
    "data['TOT_RCHG_CNT'] = pd.to_numeric(data['TOT_RCHG_CNT'])\n",
    "data['MONTHLY_PKG_COUNT'] = pd.to_numeric(data['MONTHLY_PKG_COUNT'])\n",
    "data['TOT_RCHG_CNT'] = pd.to_numeric(data['TOT_RCHG_CNT'])\n",
    "data['TOT_RCHG_CNT'] = pd.to_numeric(data['TOT_RCHG_CNT'])\n",
    "data['TOT_RCHG_CNT'] = pd.to_numeric(data['TOT_RCHG_CNT'])\n",
    "\n",
    "#Packages\n",
    "#MONTHLY_PKG_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c8153-7ef4-415c-b000-1829c849720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f967a6ec-9428-4659-972e-71ab79777e93",
   "metadata": {},
   "source": [
    "### label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651f510-4a45-47f6-bcf9-9216d56caffb",
   "metadata": {},
   "source": [
    "'PROVINCE'\n",
    "??????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861c938-1df3-4feb-b797-40bf81263d26",
   "metadata": {},
   "source": [
    "## The distribution of a data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5edbf-276c-4379-967e-bacfad25345b",
   "metadata": {},
   "source": [
    "Plotting the histogram of each numerical variable (in this case, all features), the main idea here is to visualize the data distribution for each feature. This method can bring fast insights as:\n",
    "- Check the kind of each feature distribution\n",
    "- Check data symmery\n",
    "- Verify features frequency\n",
    "- Identify outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf209c-eb1b-40e8-a139-84f9912caaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.set(style='white', font_scale=1.3, rc={'figure.figsize':(30,30)})\n",
    "data2=data.drop('CUSTOMER_ID',axis=1)\n",
    "ax=data2.hist(bins=20, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afffbbaa-1cef-4f59-9615-692ab6e8ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.plot(kind='box', subplots=True, layout=(15,15), sharex=False, sharey=False, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc2f68-1252-486b-88ee-6a5ba7bfbec1",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e5c60-92d0-4c38-9fbe-515d80c22a83",
   "metadata": {},
   "source": [
    "# Remove Outlier / Anomaly Detection\n",
    "- Standard Deviation Method: If we know that the distribution of values in the sample is Gaussian or Gaussian-like\n",
    "- Interquartile Range Method: Not all data is normal or normal enough to treat it as being drawn from a Gaussian distribution.\n",
    "- Automatic Outlier Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a109ec-6e94-4ac3-a6dd-38affe8b4d82",
   "metadata": {},
   "source": [
    "The Python Outlier Detection (PyOD) module makes your anomaly detection modeling easy. It collects a wide range of techniques ranging from supervised learning to unsupervised learning techniques.\n",
    "PyOD boasts a set of more than 30 detection algorithms, ranging from from classical algorithms like isolation forest to the latest deep learning methods to emerging algorithms like COPOD (paper). PyOD algorithms are well-established, highly cited in the literature, and useful.\n",
    "\n",
    "\n",
    "Outlier Detection Algorithms used in PyOD\n",
    "\n",
    "Let’s see the outlier detection algorithms that power PyOD. It’s well and good implementing PyOD but I feel it’s equally important to understand how it works underneath. This will give you more flexibility when you’re using it on a dataset.\n",
    "\n",
    "Note: We will be using a term Outlying score in this section. It means every model, in some way, scores a data point than uses threshold value to determine whether the point is an outlier or not.\n",
    "\n",
    " \n",
    "Angle-Based Outlier Detection (ABOD)\n",
    "\n",
    "    It considers the relationship between each point and its neighbor(s). It does not consider the relationships among these neighbors. The variance of its weighted cosine scores to all neighbors could be viewed as the outlying score\n",
    "    ABOD performs well on multi-dimensional data\n",
    "    PyOD provides two different versions of ABOD:\n",
    "        Fast ABOD: Uses k-nearest neighbors to approximate\n",
    "        Original ABOD: Considers all training points with high-time complexity\n",
    "\n",
    " \n",
    "k-Nearest Neighbors Detector\n",
    "\n",
    "    For any data point, the distance to its kth nearest neighbor could be viewed as the outlying score\n",
    "    PyOD supports three kNN detectors:\n",
    "        Largest: Uses the distance of the kth neighbor as the outlier score\n",
    "        Mean: Uses the average of all k neighbors as the outlier score\n",
    "        Median: Uses the median of the distance to k neighbors as the outlier score\n",
    "\n",
    " \n",
    "Isolation Forest\n",
    "\n",
    "    It uses the scikit-learn library internally. In this method, data partitioning is done using a set of trees. Isolation Forest provides an anomaly score looking at how isolated the point is in the structure. The anomaly score is then used to identify outliers from normal observations\n",
    "    Isolation Forest performs well on multi-dimensional data\n",
    "\n",
    " \n",
    "Histogram-based Outlier Detection\n",
    "\n",
    "    It is an efficient unsupervised method which assumes the feature independence and calculates the outlier score by building histograms\n",
    "    It is much faster than multivariate approaches, but at the cost of less precision\n",
    "\n",
    " \n",
    "Local Correlation Integral (LOCI)\n",
    "\n",
    "    LOCI is very effective for detecting outliers and groups of outliers. It provides a LOCI plot for each point which summarizes a lot of the information about the data in the area around the point, determining clusters, micro-clusters, their diameters, and their inter-cluster distances\n",
    "    None of the existing outlier-detection methods can match this feature because they output only a single number for each point\n",
    "\n",
    " \n",
    "Feature Bagging\n",
    "\n",
    "    A feature bagging detector fits a number of base detectors on various sub-samples of the dataset. It uses averaging or other combination methods to improve the prediction accuracy\n",
    "    By default, Local Outlier Factor (LOF) is used as the base estimator. However, any estimator could be used as the base estimator, such as kNN and ABOD\n",
    "    Feature bagging first constructs n sub-samples by randomly selecting a subset of features. This brings out the diversity of base estimators. Finally, the prediction score is generated by averaging or taking the maximum of all base detectors\n",
    "\n",
    " \n",
    "Clustering Based Local Outlier Factor\n",
    "\n",
    "    It classifies the data into small clusters and large clusters. The anomaly score is then calculated based on the size of the cluster the point belongs to, as well as the distance to the nearest large cluster\n",
    "\n",
    " \n",
    "Extra Utilities provided by PyOD\n",
    "\n",
    "    A function generate_data can be used to generate random data with outliers. Inliers data is generated by a multivariate Gaussian distribution and outliers are generated by a uniform distribution.\n",
    "    We can provide our own values of outliers fraction and the total number of samples that we want in our dataset. We will use this utility function to create data in the implementation part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d25eea-b18b-45fd-b16e-3945f6ecd124",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data.drop(['CUSTOMER_ID','ACTIVE','ACTIVE_DATA_RIAL','ACTIVE_DATA_USAGE',\"ACTIVE\",\"ACTIVE_DATA_RIAL\",\"ACTIVE_DATA_USAGE\",\"ACTIVE_VOICE_RIAL\",\"ACTIVE_VOICE_USAGE\", \"ACTIVE_SMS_RIAL\",\n",
    "                \"ACTIVE_SMS_USAGE\",\"ACTIVE_INTEC_USAGE\", \"ACTIVATION_YEAR\",\"ACTIVATION_MONTH\",'PROVINCE','SIM_PLAN_CHANNEL','SIM_PLAN_CATEGORY','SEGMENT_NEW_VER','VOICE_MAIN_LACCELL',\n",
    "                'VOICE_MAIN_LAC_CELL_PROVINCE', \"VOL_2G_CBS_BYTE\",\"VOL_3G_CBS_BYTE\",\"VOL_4G_CBS_BYTE\",\"VOL_UNK_CBS_BYTE\",'PROVINCE'], axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6efce7-5aa7-46ec-8833-b365064fef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"ARPU_RIAL\", y=\"ARPU_NONDATA\", data=data)\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec178f16-1e54-4fc9-b479-d0be48f891a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a8d08-a56d-427d-81e6-397407ed0b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5146d4-2bb3-483e-a403-69759a0e53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"ARPU_RIAL\", y=\"ARPU_NONDATA\", data=data)\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63813fea-28c0-439e-86ae-84a7464302ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee12736-5e4f-4f71-b7dc-a900a1f38230",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = data.columns.values\n",
    "number_of_columns=60\n",
    "number_of_rows = len(l)-1/number_of_columns\n",
    "\n",
    "plt.figure(figsize=(number_of_columns,5*number_of_rows))\n",
    "for i in range(0,len(l)):\n",
    "    plt.subplot(number_of_rows + 1, number_of_columns,i+1)\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.boxplot(data[l[i]],color='green',orient='v')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a45ece-bc10-4709-8e68-675313558bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[l[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063136d9-40e0-4d48-9bdc-67a5b424c6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38ad50ca-e3cd-45cb-b879-0c00cd2d9770",
   "metadata": {},
   "source": [
    "- Distribution-based techniques – Minimum Covariance Determinant, Elliptic Envelope\n",
    "- Depth-based technique – Isolation Forest\n",
    "- Clustering-based technique – Local Outlier Factor\n",
    "- Density-based technique – DBSCAN\n",
    "- Unified library for Outlier Detection – PyOD\n",
    "- Statistical techniques – Interquartile range\n",
    "- Visualization techniques – Box-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45cc9fe-7fce-478a-a492-bd4b890341cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.4 , random_state=42)\n",
    "\n",
    "# Make the 2d numpy array a pandas dataframe for each manipulation \n",
    "X_train_pd = pd.DataFrame(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188df99-70de-442e-966c-f5c28c36c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train kNN detector\n",
    "from pyod.models.knn import KNN\n",
    "clf_name = 'KNN'\n",
    "clf = KNN()\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd25bc9-5568-40fc-90a5-2132c4e6ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "y_test_scores = clf.decision_function(X_test)  # outlier scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a913f0-9c7a-4e9d-a787-55ab724139c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train_pred[y_train_pred==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013325d-b832-4956-8a5a-f3755247c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test_pred[y_test_pred==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f63e72-7d1c-4cdd-8879-c5b55d17cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have the trained K-NN model, let's apply to the test data to get the predictions\n",
    "y_test_pred = clf.predict(X_test) # outlier labels (0 or 1)\n",
    "# Because it is '0' and '1', we can run a count statistic. There are 44 '1's and 456 '0's. The number of anomalies is roughly ten percent, as we have generated before:\n",
    "unique, counts = np.unique(y_test_pred, return_counts=True)\n",
    "dict(zip(unique, counts))\n",
    "#{0: 456, 1: 44}\n",
    "# And you can generate the anomaly score using clf.decision_function:\n",
    "y_test_scores = clf.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039db80-9fcc-4a7e-9338-07216ecaa389",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd0d78-856f-4bed-8509-8f5d15b577a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d9545d-da81-41b7-886e-bb263cb31036",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa75ca2-57fe-449a-addf-d681e505adc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b878ca-69cf-491f-afb2-0c84f1f2a7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5950cb-fdaf-4469-9fc6-80b3b0b27d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test_scores, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram with 'auto' bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709afe5-665e-42ca-8b9b-c0ca824917a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many '0's and '1's. We get 452 '0's and 48 '1's.\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_test['score'] = y_test_scores\n",
    "df_test['cluster'] = np.where(df_test['score']<1, 0, 1)\n",
    "df_test['cluster'].value_counts()\n",
    "\n",
    "# Now let's show the summary statistics:\n",
    "# df_test.groupby('y_by_average_cluster').mean()\n",
    "df_test.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c0d16-3907-41a7-a831-4a9cae3978b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314673fb-4490-4dc5-8aeb-d84fe6306e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.query('cluster==0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd52ff5-9af4-4a71-bdaa-5045e09812c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[39835,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b99bd3-342d-4c4f-81d1-d74c6c237d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8bb6b-0f7c-46a2-9b39-d72f9f48a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.scatter(X_train_pd[0], X_train_pd[1], c=y_train, alpha=0.8)\n",
    "plt.title('Scatter plot pythonspot.com')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11339ced-b713-40da-af34-ca64aebb1fb0",
   "metadata": {},
   "source": [
    "### Inter Quartile Range(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce189d22-e2bd-4572-b529-be6135922201",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = data.ARPU_DATA\n",
    "quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n",
    "iqr = quartile_3 - quartile_1\n",
    "lower_bound = quartile_1 - (iqr * 1.5)\n",
    "upper_bound = quartile_3 + (iqr * 1.5)\n",
    "outliers_indices = np.where((ys > upper_bound) | (ys < lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f8761-0a8d-4554-bc1b-bec46c4b9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7b67c-59ca-463b-bd52-b1f51bf3c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    " np.percentile(ys, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a38320-9d41-4884-8fb5-95f8715ce84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ARPU_DATA.value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12aa387-9bf5-46b4-bf35-5e519e4c0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9c254-2f22-4e29-9c33-325677ce0a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd65f82-e4fc-49b5-80d6-192cfc453cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outliers_fraction = 0.01\n",
    "xx , yy = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
    "clf = CBLOF(contamination=outliers_fraction,check_estimator=False, random_state=0)\n",
    "clf.fit(X)\n",
    "scores_pred = clf.decision_function(X) * -1\n",
    "y_pred = clf.predict(X)\n",
    "n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "n_outliers = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "df1 = data\n",
    "df1['outlier'] = y_pred.tolist()\n",
    "    \n",
    "# sales - inlier feature 1,  profit - inlier feature 2\n",
    "inliers_sales = np.array(df1['Sales'][df1['outlier'] == 0]).reshape(-1,1)\n",
    "inliers_profit = np.array(df1['Profit'][df1['outlier'] == 0]).reshape(-1,1)\n",
    "    \n",
    "# sales - outlier feature 1, profit - outlier feature 2\n",
    "outliers_sales = df1['Sales'][df1['outlier'] == 1].values.reshape(-1,1)\n",
    "outliers_profit = df1['Profit'][df1['outlier'] == 1].values.reshape(-1,1)\n",
    "         \n",
    "print('OUTLIERS:',n_outliers,'INLIERS:',n_inliers)\n",
    "threshold = percentile(scores_pred, 100 * outliers_fraction)        \n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),cmap=plt.cm.Blues_r)\n",
    "a = plt.contour(xx, yy, Z, levels=[threshold],linewidths=2, colors='red')\n",
    "plt.contourf(xx, yy, Z, levels=[threshold, Z.max()],colors='orange')\n",
    "b = plt.scatter(inliers_sales, inliers_profit, c='white',s=20, edgecolor='k')\n",
    "    \n",
    "c = plt.scatter(outliers_sales, outliers_profit, c='black',s=20, edgecolor='k')\n",
    "       \n",
    "plt.axis('tight')   \n",
    "plt.legend([a.collections[0], b,c], ['learned decision function', 'inliers','outliers'],\n",
    "           prop=matplotlib.font_manager.FontProperties(size=20),loc='lower right')      \n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 1))\n",
    "plt.title('Cluster-based Local Outlier Factor (CBLOF)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df12707-477f-4be1-9806-27d366479e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8da8283d-bd68-43e3-bd83-e752c6595e04",
   "metadata": {},
   "source": [
    "### Manualy Outlier and Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc15815-b90e-4c41-9fb5-5b4a40f9dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(data.shape[0]),data['COUNT_MSISDN'].values)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('COUNT MSISDN')\n",
    "plt.title('Distribution of COUNT MSISDN')\n",
    "sns.despine() #Remove the top and right spines from plot(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11cc56-74f5-41eb-bf52-9d1aff10cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "back=data.copy()\n",
    "back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877d391-99d1-4a6d-9b53-1def51b1e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data.query('COUNT_MSISDN > 10')))\n",
    "out=data.query('COUNT_MSISDN > 10')\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea60f5-141e-44d1-997b-3bf1fb92c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(out.index, axis=0 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b110111-4fa5-46ac-9203-6ec39326fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data.query('ARPU_RIAL <0')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4ee71-156d-4c53-b2b2-653de0bf4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data.ARPU_RIAL<0].index,axis=0 , inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03654b63-604e-43b9-a8f6-22b163db8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ARPU_RIAL'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37fdf9-ea72-4d69-91d4-f49c752c980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(data.shape[0]), np.sort(data['ARPU_RIAL'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf33a67-55c8-4008-b221-cf17af0a1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['ARPU_RIAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71a993-78a3-4140-965b-f0965741bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_forest = IsolationForest(n_estimators=100)\n",
    "isolation_forest.fit(data['ARPU_RIAL'].values.reshape(-1, 1))\n",
    "xx = np.linspace(data['ARPU_RIAL'].min(), data['ARPU_RIAL'].max(), len(data)).reshape(-1,1)\n",
    "anomaly_score = isolation_forest.decision_function(xx)\n",
    "outlier = isolation_forest.predict(xx)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(xx, anomaly_score, label='anomaly score')\n",
    "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score), where=outlier==-1, color='r',  alpha=.4, label='outlier region')\n",
    "plt.legend()\n",
    "plt.ylabel('anomaly score')\n",
    "plt.xlabel('ARPU_RIAL')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d291dd-4a41-43bf-9988-97537378f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = data.ARPU_RIAL\n",
    "quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n",
    "iqr = quartile_3 - quartile_1\n",
    "lower_bound = quartile_1 - (iqr * 1.5)\n",
    "upper_bound = quartile_3 + (iqr * 1.5)\n",
    "outliers_indices = np.where((ys > upper_bound) | (ys < lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b6d8f-f12e-4cf6-abe0-e2c58c6e45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5de667-3238-4980-830f-cf9efc20a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(ys > upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309b01f-4661-4fd6-b287-c0ab3e6cc7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ARPU_RIAL.sort_values(ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682f15c-74fb-4cb0-b19d-32585a20e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[26086,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67e229-46c2-4bdc-af5d-9c6cda673e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outliers_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6443a286-213b-4dfa-881a-f656bedc1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(outliers_indices[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77bc72-0a0d-4f60-b5f0-79edfa32af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856debd4-6bfe-4255-852f-1b20445ec7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(data.shape[0]), np.sort(data['ARPU_RIAL'].values))\n",
    "data['ARPU_RIAL']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe4829-7cb2-45c5-8c4a-7bb041296902",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=data.AGE, y=data.COUNT_MSISDN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63302d83-ebf7-4e62-bfb5-4e1f6b1b463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(data.shape[0]),data['POSTPAID_COUNTS'].values)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('POSTPAID COUNTS')\n",
    "plt.title('Distribution of  POSTPAID COUNTS')\n",
    "sns.despine() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc984397-891f-4ff4-85c1-ccf032654922",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=data.AGE, y=data.POSTPAID_COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88ab95-bf3c-4b5d-889c-ea2ffe15df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "len(data.query('AGE < 18'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c211896-df82-433b-a3a3-e37591ac2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(data.shape[0]),data['PREPAID_COUNTS'].values)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('PREPAID COUNTS')\n",
    "plt.title('Distribution of  PREPAID COUNTS')\n",
    "sns.despine() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045ebb9-d17b-4796-ba1e-8aea38f850f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.query('PREPAID_COUNTS > 10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8a411-a52f-4c64-9f6a-4d1ef0c2322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"skewness: %f\" % data['ARPU_DATA'].skew())# unbiased skew over requested axis.\n",
    "print(\"kurtosis: %f\" % data['ARPU_DATA'].kurt()) # unbiased kurtosis over requested axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a902ca-c932-46ad-a430-c4705b741aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(data.shape[0]),data['ARPU_NONDATA'].values)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('ARPU NONDATA')\n",
    "plt.title('Distribution of ARPU NONDATA')\n",
    "sns.despine() #Remove the top and right spines from plot(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c427c78-5621-4545-93cd-9596fce370fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data, x='COUNT_MSISDN', hue='GENDER', kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba891b4-aef9-41f4-94a4-de49a6bc7eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "189bbdac-d764-41d2-a20c-2e9848f17e28",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction \n",
    "- PCA\n",
    "- UMAP\n",
    "- T-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3cb526-edeb-483a-8b05-f9a85d222778",
   "metadata": {},
   "source": [
    "### PCA(Principal Component Analysis)\n",
    "PCA is effected by scale so you need to scale the features in your data before applying PCA. Use StandardScaler to help you standardize the dataset’s features onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. If you want to see the negative effect not scaling your data can have, scikit-learn has a section on the effects of not standardizing your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62762e-4eb3-4264-80fb-6ee112a9469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data.drop(['CUSTOMER_ID','ACTIVE','ACTIVE_DATA_RIAL','ACTIVE_DATA_USAGE',\"ACTIVE\",\"ACTIVE_DATA_RIAL\",\"ACTIVE_DATA_USAGE\",\"ACTIVE_VOICE_RIAL\",\"ACTIVE_VOICE_USAGE\", \"ACTIVE_SMS_RIAL\",\n",
    "           \"ACTIVE_SMS_USAGE\",\"ACTIVE_INTEC_USAGE\", \"ACTIVATION_YEAR\",\"ACTIVATION_MONTH\",'PROVINCE','SIM_PLAN_CHANNEL','SIM_PLAN_CATEGORY','SEGMENT_NEW_VER','VOICE_MAIN_LACCELL','VOICE_MAIN_LAC_CELL_PROVINCE',\n",
    "           \"VOL_2G_CBS_BYTE\",\"VOL_3G_CBS_BYTE\",\"VOL_4G_CBS_BYTE\",\"VOL_UNK_CBS_BYTE\"], axis=1)\n",
    "data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d068ff-7422-4b0d-a9eb-d391efdca45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cf4f4-2912-4538-93c6-be6733a8c4ea",
   "metadata": {},
   "source": [
    "### ploting 65 dimesions into 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1ebdd-e068-4a10-8b30-85af33db185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating out the features\n",
    "x = data2.loc[:, data2.columns].values\n",
    "# Separating out the target\n",
    "#y = df.loc[:,['target']].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ce1f55-67a4-4121-a95c-3f38c22baffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=2)\n",
    "pcComponents=pca.fit_transform(x)\n",
    "pcDF=pd.DataFrame(data=pcComponents, columns=['pc1','pc2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e8c77-5423-40b1-a52f-5e9bb05cffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(pcComponents[:,0],pcComponents[:,1], edgecolors='none',alpha=.7, s=40, cmap=plt.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2035258-759a-48f8-b6e4-17dae5ea4ebf",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab584a-d55e-4173-9d61-4e76e1252937",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Clustering is the task of grouping a set of objects in such a way that those in the same group (called a cluster) are more similar to each other than to those in other groups. It is an exploratory data mining activity, and a common technique for statistical data analysis used in many fields including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression and computer graphics. Some common real life use cases of clustering are:\n",
    "\n",
    "- Customer segmentation based on purchase history or interests to design targetted marketing compaigns.\n",
    "- Cluster documents into multiple categories based on tags, topics, and the content of the document.\n",
    "- Analysis of outcome in social / life science experiments to find natural groupings and patterns in the data.\n",
    "\n",
    "### A list of 10 of the more popular algorithms is as follows:\n",
    "\n",
    "- Affinity Propagation\n",
    "- Agglomerative Clustering\n",
    "- BIRCH\n",
    "- DBSCAN\n",
    "- K-Means\n",
    "- Mini-Batch K-Means\n",
    "- Mean Shift\n",
    "- OPTICS\n",
    "- Spectral Clustering\n",
    "- Mixture of Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbd7c6-2abe-4ae8-b2a6-f94cc7029e6c",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "a) By dropping features which are very similar to each other and keep just one out of the two.\n",
    "b) By combining features which represent more sensible information when considered together. and if you cannot afford to do either of the two or even after applying them, the dimensions are a mess we then can\n",
    "c) Use any of the several available dimensionality reduction techniques.\n",
    "\n",
    "\n",
    "We all know that KMeans is great, that but it does not work well with higher dimension data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c05aa-97d1-45cf-9776-8294cb4c5ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "arpu_ds=data[['ARPU_DATA','ARPU_NONDATA']]\n",
    "ss=StandardScaler()\n",
    "data_cluster=arpu_ds.copy()\n",
    "data_cluster[data_cluster.columns]=ss.fit_transform(data_cluster)\n",
    "data_cluster.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145dd949-614e-4817-9549-4640835306a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data[['ARPU_DATA','ARPU_NONDATA']].values\n",
    "wcss=[]\n",
    "for i in range(1,10):\n",
    "    kmeans=KMeans(n_clusters=i,init='k-means++', random_state=42)\n",
    "    kmeans.fit(x)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa2b3e-bc9e-453f-b8aa-902c561d3a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "plt.plot(range(1,10),wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe911361-1b90-4ac8-8ff7-8c3262e2ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans=KMeans(n_clusters=4,init='k-means++',random_state=42)\n",
    "y_kmeans=kmeans.fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710af813-0187-4eea-91ab-8600b19bd9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:,0] ,x[:,1] , c=y_kmeans, s=50, cmap='viridis' )\n",
    "centers=kmeans.cluster_centers_\n",
    "plt.scatter(centers[:,0], centers[:,1], c='black', s=200, alpha=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa630c13-c0bb-4b43-8139-b339e8162bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde51252-223a-4fa2-9b39-4ed22b7756b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
